{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FICTURE documentation","text":""},{"location":"#what-is-ficture","title":"What is FICTURE?","text":"<p>FICTURE is a software tool that performs segmentation-free analysis of submicron-resolution analysis of spatial transcriptomics data.</p> <p>In brief, FICTURE...</p> <ul> <li>identifies cell types and disease-specific tissue microenvironments from high-resolution (&lt;1\u00b5m) spatial transcriptomics (ST) data without compromising its original resolution.</li> <li>is compatible with almost all major high-resolution ST platforms including Seq-Scope, Stereo-seq, 10x Xenium, 10x Visium HD, Vizgen MERSCOPE, CosMx SMI, Pixel-seq, OpenST, and Nova-ST. </li> <li>does NOT require externally provided cell segmentation or histology. </li> <li>works in all-in-one mode (unsupervised clustering + pixel-level decoding) or projection mode (pixel-level decoding from celltypes/clusters identified by external tools or datasets).</li> <li>successfully identified fine-scale tissue architectures for challenging tissues where existing methods have struggled, including vascular, fibrotic, muscular and lipid-laden areas.</li> </ul> <p>For more details, please refer to the following publication:</p> <p>Si, Y., Lee, C., Hwang, Y. et al. FICTURE: scalable segmentation-free analysis of submicron-resolution spatial transcriptomics. Nat Methods 21, 1843\u20131854 (2024). https://doi.org/10.1038/s41592-024-02415-2</p>"},{"location":"#documentation-overview","title":"Documentation Overview","text":"<p>This documentation provides several instructions on how to install and run FICTURE on your data in various details.</p> <ul> <li>Quick start gives a quick walkthrough of installing and running FICTURE on an example data.</li> <li>Small run provides detailed examples of executing individual commands on a local machine.</li> <li>Run describes how to run FICTURE for a large dataset in a linux environment, including instructions of  submitting jobs via SLURM.</li> <li>Format input describes how to prepare the input files from different platforms' raw output.</li> </ul>"},{"location":"#important-notes-in-preparing-input-data","title":"Important Notes in Preparing Input Data","text":"<p>FICTURE is compatible with various high-resolution spatial transcriptomics platforms as detailed in the Format input section, but in essence, a TSV file that contains the spatial coordinates and gene expression values is what we need. Here are some key considerations when preparing the input data:</p> <ol> <li>Coordinate unit in micrometer. FICTURE recommends using micrometer unit for spatial coordinates (X and Y) in your input file. Depending on your data generation platform, the spatial coordinates in your raw input file may not be in micrometer units. If your input data is not a micrometer unit, you would need to either (1) convert your input data into micrometer scale, or (2) set the translation ratio in a parameter <code>mu_scale</code>. See Format input for more details.</li> <li>Sort your input by coordinates. To scale to really large datasets, we assume the input data is sorted according to one axis, either X or Y, and you would need to let the software know by setting <code>major_axis</code>.</li> <li>Bounding box of spatial coordinates. To visualize the final high resolution pixel level result, we would need to tell the software the minimum and maximum values of the coordinates, in micrometer. See Format input for more details.</li> </ol>"},{"location":"format_input/","title":"Format raw output from different platforms","text":"<p>FICTURE only needs a file with 2D coordinates, gene/transcript ID, and the corresponding transcript count.</p> <p>We have tested FICTURE on Seq-scope, Stereo-seq, CosMx SMI, Xenium, MERSCOPE, and Visium HD data. Here we document how to format the raw data from each platform to the required input files like the following example data.</p> <pre><code>examples/data/transcripts.tsv.gz\nexamples/data/feature.clean.tsv.gz\n</code></pre> <p>CosMx SMI</p> <p>10X Xenium</p> <p>10X Visium HD</p> <p>Vizgen MERSCOPE</p>"},{"location":"install/","title":"Installing FICTURE","text":""},{"location":"install/#installing-from-github-repository","title":"Installing from GitHub repository","text":"<p>To install the latest copy of FICTURE, you can clone the repository from GitHub.</p> <pre><code>git clone https://github.com/seqscope/ficture.git\n</code></pre> <p>It is recommended to create a virtual environment and install FICTURE and its dependencies.</p> <p>FICTURE is tested with python 3.11 and 3.9. While there might be problems with different python versions, the required packages are fairly standard, so we expect it to work with other versions of python as well.</p> <p>To follow our best practices, it is recommended to use python 3.11 and update pip to the newest version.</p> <p>(<code>requirements.txt</code> is at the root directory of FICTURE)</p> <p>Here are example commands to install FICTURE and its dependencies.</p> <pre><code>## Create a virtual environment\nVENV=/path/to/venv/name   ## replace /path/to/venv/name with your desired path\npython -m venv ${VENV}\n\n## Activate the virtual environment\nsource ${VENV}/bin/activate\n\n## Clone the GitHub repository\ngit clone https://github.com/seqscope/ficture.git\ncd ficture\n\n## Install the required packages\npip install -r requirements.txt\n\n## Install FICTURE locally\npip install -e .\n</code></pre> <p>Installing FICTURE this way allows you to run the software while making minor changes to the codebase. It also allows you to access the example data and scripts that come with the repository.</p>"},{"location":"install/#installing-from-pypi","title":"Installing from PyPI","text":"<p>If you want to install FICTURE from the pypi repository, you can do so with the following command.</p> <pre><code>pip install ficture\n</code></pre> <p>Note that the version on PyPI may not be the latest version of FICTURE. Sometimes the <code>dev</code> branch main contain some of the latest features that may not have been merged with the <code>main</code> branch. </p>"},{"location":"localrun/","title":"Running FICTURE in a local machine","text":""},{"location":"localrun/#overview","title":"Overview","text":"<p>This document provides detailed instructions on how to run FICTURE on a local machine with real data. This instruction is intended for Ubuntu OS, but it should also work for Mac OS X and other Unix-like systems. If you rather want to run all steps together with <code>run_togetehr</code> command, please refer to Quick start for details.</p>"},{"location":"localrun/#setup","title":"Setup","text":""},{"location":"localrun/#input-data","title":"Input Data","text":"<p>A small sub-region of Vizgen MERSCOPE mouse liver data is provided as an example in the GitHub repository</p> <p><pre><code>## main transcript file\nexamples/data/transcripts.tsv.gz\n## gene list file (optional)\nexamples/data/feature.clean.tsv.gz\n## bounding box file (optional)\nexamples/data/coordinate_minmax.tsv\n</code></pre> (All filese are tab-delimited text files unless specified otherwise.)</p> <p>See the following explanation for each file. If you have trouble, try Format input to see some examples of formatting raw output from different platforms.</p>"},{"location":"localrun/#transcript-file","title":"Transcript file","text":"<p>One file contains the molecular or pixel level information, the required columns are <code>X</code>, <code>Y</code>, <code>gene</code>, and <code>Count</code>. (There could be other columns in the file which would be ignored.)</p> <p>The coordinates <code>(X, Y)</code> can be float or integer numbers in arbitrary units, but if it is not in the unit of \\(\\mu m\\) we would need to specify the translation ratio later.</p> <p>The file has to be sorted by one of the coordinates. (Usually it is the longer axis, but it does not matter if the tissue area is not super asymmetric.)</p> <p><code>Count</code> (could be any other name) is the number of transcripts for the specified <code>gene</code> observed at the coordinate. For imaging based technologies where each molecule has its unique coordinates, <code>Count</code> could be always 1.</p>"},{"location":"localrun/#gene-list-file","title":"Gene list file","text":"<p>Another file contains the (unique) names of genes that should be used in analysis. The required columns is just <code>gene</code> (including the header), the naming of genes should match the <code>gene</code> column in the transcript file. If your data contain negative control probes or if you would like to remove certain genes this is where you can specify. (If you would like to use all genes present in your input transcript file the gene list is not necessary, but you would need to modify the command in <code>examples/script/generic_III.sh</code> to remove the argument <code>--feature</code> )</p>"},{"location":"localrun/#bounding-box-of-spatial-coordinates","title":"Bounding box of spatial coordinates","text":"<p>We also prefer to keep a file listing the min and max of the coordinates (this is primarily for visualizing very big tissue region where we do not read all data at once but would want to know the image dimension). The unit of the coordinates is micrometer.</p> <pre><code>examples/data/coordinate_minmax.tsv\n</code></pre> <p>Note that, when <code>run_together</code> command is used, the gene list file and bounding box files will be automatically generated.</p>"},{"location":"localrun/#prepare-environment","title":"Prepare environment","text":"<p>Activate your virtual environment if needed:</p> <pre><code>VENV=/path/to/venv/name   ## replace /path/to/venv/name with your virtual environment path\nsource ${VENV}/bin/activate\n</code></pre> <p>Suppose you have installed FICTURE and dependencies following Install in this environment. Verify FICTURE is successfully installed with command <code>ficture</code>.</p>"},{"location":"localrun/#analysis-with-ficture","title":"Analysis with FICTURE","text":""},{"location":"localrun/#key-parameters","title":"Key parameters","text":"<p>First, specify the base directory that contains the input data</p> <pre><code>path=examples/data\n</code></pre> <p>The following data-specific setup may be required:</p> <ul> <li><code>mu_scale</code> is the ratio between \\(\\mu m\\) and the unit used in the transcript coordinates. For example, if the coordinates are stored in <code>nm</code> this number should be <code>1000</code>.</li> <li><code>key</code> is the column name in the transcripts file corresponding to the gene counts (<code>Count</code> in our example).</li> <li><code>major_axis</code> specify which axis the transcript file is sorted by. (either <code>X</code> or <code>Y</code>)</li> </ul> <pre><code>mu_scale=1   # If your data's coordinates are already in micrometer\nkey=Count    # If you data has 'Count' as the column name for gene counts\nmajor_axis=Y # If your data is sorted by the Y-axis\n</code></pre>"},{"location":"localrun/#preprocessing","title":"Preprocessing","text":""},{"location":"localrun/#pixel-level-minibatch","title":"Pixel-level minibatch","text":"<p>Create pixel minibatches (<code>${path}/batched.matrix.tsv.gz</code>) that will be used for pixel-level analysis using the following command:</p> <pre><code>batch_size=500\nbatch_buff=30\ninput=${path}/transcripts.tsv.gz\noutput=${path}/batched.matrix.tsv.gz\nbatch=${path}/batched.matrix.tsv\n\nficture make_spatial_minibatch --input ${input} --output ${batch} --mu_scale ${mu_scale} --batch_size ${batch_size} --batch_buff ${batch_buff} --major_axis ${major_axis}\ngzip -f ${batch}\n</code></pre>"},{"location":"localrun/#training-hexagons","title":"Training hexagons","text":"<p>Prepare training hexagons. Even if you need to fit multiple models with different number of factors, you only need to run once for each training width. The training width is the flat-to-flat width of the hexagon in \\(\\mu m\\).</p> <pre><code>## set up the parameters\ntrain_width=12      # flat-to-flat width = \\sqrt{3} x the side length of the hexagon (um)\nmin_ct_per_unit=50  # filter out hexagons with total count &lt; 50\ninput=${path}/transcripts.tsv.gz\nout=${path}/hexagon.d_${train_width}.tsv\n\n## create hexagons\nficture make_dge --key ${key} --count_header ${key} --input ${input} --output ${out} --hex_width ${train_width} --n_move 2 --min_ct_per_unit ${min_ct_per_unit} --mu_scale ${mu_scale} --precision 2 --major_axis ${major_axis}\n\n## shuffle the hexagons based on random index\nsort -S 4G -k1,1n ${out} | gzip -c &gt; ${out}.gz\nrm ${out}\n</code></pre>"},{"location":"localrun/#lda-model-training","title":"LDA Model training","text":"<p>To run FICTURE in a fully unsupervised manner, you need to initialize the model with LDA based on the hexagons created in the previous step.</p>"},{"location":"localrun/#parameters-for-initializing-the-model","title":"Parameters for initializing the model","text":"<pre><code>nFactor=12 # Number of factors\nsliding_step=2\ntrain_nEpoch=3\n# train_width=12 # should be the same as used in the above step\nmodel_id=nF${nFactor}.d_${train_width} # An identifier kept in output file names\nmin_ct_per_feature=20 # Ignore genes with total count \\&lt; 20\nR=10 # We use R random initializations and pick one to fit the full model\nthread=4 # Number of threads to use\n</code></pre>"},{"location":"localrun/#setting-the-input-and-output-paths","title":"Setting the input and output paths","text":"<pre><code># parameters\nmin_ct_per_unit_fit=20\ncmap_name=\"turbo\"\n\n# output identifiers\nmodel_id=nF${nFactor}.d_${train_width}\noutput_id=${model_id}\noutput_path=${path}/analysis/${model_id}\nfigure_path=${output_path}/figure\nif [ ! -d \"${figure_path}/sub\" ]; then\n    mkdir -p ${figure_path}/sub\nfi\n\n# input files\nhexagon=${path}/hexagon.d_${train_width}.tsv.gz\npixel=${path}/transcripts.tsv.gz\nfeature=${path}/feature.clean.tsv.gz\n# output\noutput=${output_path}/${output_id}\nmodel=${output}.model.p\n</code></pre>"},{"location":"localrun/#initialize-the-model-with-lda","title":"Initialize the model with LDA","text":"<pre><code># Fit model with unsupervised LDA\nficture fit_model --input ${hexagon} --output ${output} --feature ${feature} --nFactor ${nFactor} --epoch ${train_nEpoch} --epoch_id_length 2 --unit_attr X Y --key ${key} --min_ct_per_feature ${min_ct_per_feature} --test_split 0.5 --R ${R} --thread ${thread}\n</code></pre>"},{"location":"localrun/#optional-initializing-lda-model-from-pseudo-bulk-data","title":"(Optional) Initializing LDA model from pseudo-bulk data","text":"<p>Instead of initializing the model using LDA as shown above, if you want to initialize the model using pseudo-bulk data, you can prepare the pseudo-bulk data as a model matrix in the following TSV format in a <code>tsv.gz</code> file:</p> <pre><code>gene    celltype1   celltype2   ...\ngene1   10          20          ...\ngene2   5           15          ...\n...\n</code></pre> <p>This model matrix can be directly used for pixel-level decoding step described below. However, if the gene list do not match between the pseudo-bulk data and the raw data, you may need to use the following command to initialize the model from the pseudo-bulk data.</p> <pre><code># Fit model from pseudo-bulk data\nficture init_model_from_pseudobulk --input ${hexagon} --output ${output} --feature ${feature} --epoch 0 --scale_model_rel -1 --reorder-factors --key ${key} --min_ct_per_feature ${min_ct_per_feature}--thread ${thread}\n\n# create a model matrix from the posterior count\ncp ${output}.posterior.count.tsv.gz ${output}.model_matrix.tsv.gz\n</code></pre> <p>After running the following command, the model will be initialized using the pseudo-bulk data.</p>"},{"location":"localrun/#visualizing-the-model","title":"Visualizing the model","text":"<p>The results from the initial model fitting can be visualized using the following commands:</p> <pre><code># Choose color\ninput=${output_path}/${output_id}.fit_result.tsv.gz\noutput=${figure_path}/${output_id}\ncmap=${figure_path}/${output_id}.rgb.tsv\nficture choose_color --input ${input} --output ${output} --cmap_name ${cmap_name}\n\n# Coarse plot for inspection\ncmap=${figure_path}/${output_id}.rgb.tsv\ninput=${output_path}/${output_id}.fit_result.tsv.gz\noutput=${figure_path}/${output_id}.coarse\nfillr=$((train_width/2+1))\nficture plot_base --input ${input} --output ${output} --fill_range ${fillr} --color_table ${cmap} --plot_um_per_pixel 1 --plot_discretized\n</code></pre>"},{"location":"localrun/#pixel-level-decoding","title":"Pixel level decoding","text":""},{"location":"localrun/#parameters-for-pixel-level-decoding","title":"Parameters for pixel level decoding","text":"<p>After fitting the model, FICTURE performs pixel level decoding to infer the factors for each pixel. The pixel-level decoding consists of two steps: * Perform anchor-level projection based on the fitted model * Perform pixel-level decoding based on anchor-level projection</p> <p>The following parameters can be used for pixel level decoding steps.</p> <pre><code>fit_width=12 # Often equal or smaller than train_width (um)\nanchor_res=4 # Distance between adjacent anchor points (um)\nfit_nmove=$((fit_width/anchor_res))\nanchor_info=prj_${fit_width}.r_${anchor_res}\nradius=$(($anchor_res+1))\nanchor_info=prj_${fit_width}.r_${anchor_res} # An identifier\ncoor=${path}/coordinate_minmax.tsv\ncmap=${figure_path}/${output_id}.rgb.tsv\n</code></pre>"},{"location":"localrun/#produce-anchor-level-projection","title":"Produce anchor-level projection","text":"<p>Anchor-level projection can be performed using the following command:</p> <pre><code># Output prefix\noutput=${output_path}/${output_id}.${anchor_info}\n\n# Perform anchor-level projection\nficture transform --input ${pixel} --output_pref ${output} --model ${model} --key ${key} --major_axis ${major_axis} --hex_width ${fit_width} --n_move ${fit_nmove} --min_ct_per_unit ${min_ct_per_unit_fit} --mu_scale ${mu_scale} --thread ${thread} --precision 2\n</code></pre>"},{"location":"localrun/#perform-pixel-level-decoding","title":"Perform pixel-level decoding","text":"<p>Pixel-level decoding can be performed using the following command:</p> <pre><code># Input/output parameters for pixel-level decoding\nprefix=${output_id}.decode.${anchor_info}_${radius}\ninput=${path}/batched.matrix.tsv.gz\nanchor=${output_path}/${output_id}.${anchor_info}.fit_result.tsv.gz\noutput=${output_path}/${prefix}\ntopk=3 # Output only a few top factors per pixel\n\n# Perform pixel-level decoding\nficture slda_decode --input ${input} --output ${output} --model ${model} --anchor ${anchor} --anchor_in_um --neighbor_radius ${radius} --mu_scale ${mu_scale} --key ${key} --precision 0.1 --lite_topk_output_pixel ${topk} --lite_topk_output_anchor ${topk} --thread ${thread}\n</code></pre>"},{"location":"localrun/#optional-post-processing","title":"Optional post-processing","text":"<p>Although not required, after performing pixel-level decoding, it is useful to generates summary statistics and visualize the results.</p> <p>First step is to sort the pixel level output. This is primarily for visualizing large images with limited memory.</p> <p>We will need the range of the coordinates for visualization. You can also set <code>xmin</code>, <code>xmax</code>, <code>ymin</code>, and <code>ymax</code> directly. <pre><code>while IFS=$'\\t' read -r r_key r_val; do\n    export \"${r_key}\"=\"${r_val}\"\ndone &lt; ${coor}\necho -e \"${xmin}, ${xmax}; ${ymin}, ${ymax}\"\n</code></pre></p> <pre><code>input=${output_path}/${prefix}.pixel.tsv.gz # j, X, Y, K1, ..., KJ, P1, ..., PJ, J=topk\noutput=${output_path}/${prefix}.pixel.sorted.tsv.gz\n\nK=$( echo $model_id | sed 's/nF\\([0-9]\\{1,\\}\\)\\..*/\\1/' )\n\noffsetx=${xmin}\noffsety=${ymin}\nrangex=$( echo \"(${xmax} - ${xmin} + 0.5)/1+1\" | bc )\nrangey=$( echo \"(${ymax} - ${ymin} + 0.5)/1+1\" | bc )\nbsize=2000\nscale=100\nheader=\"##K=${K};TOPK=3\\n##BLOCK_SIZE=${bsize};BLOCK_AXIS=X;INDEX_AXIS=Y\\n##OFFSET_X=${offsetx};OFFSET_Y=${offsety};SIZE_X=${rangex};SIZE_Y=${rangey};SCALE=${scale}\\n#BLOCK\\tX\\tY\\tK1\\tK2\\tK3\\tP1\\tP2\\tP3\"\n\n(echo -e \"${header}\" &amp;&amp; zcat ${input} | tail -n +2 | perl -slane '$F[0]=int(($F[1]-$offx)/$bsize) * $bsize; $F[1]=int(($F[1]-$offx)*$scale); $F[1]=($F[1]&gt;=0)?$F[1]:0; $F[2]=int(($F[2]-$offy)*$scale); $F[2]=($F[2]&gt;=0)?$F[2]:0; print join(\"\\t\", @F);' -- -bsize=${bsize} -scale=${scale} -offx=${offsetx} -offy=${offsety} | sort -S 4G -k1,1g -k3,3g ) | gzip -c &gt; ${output}\n\n# rm ${input} # Make sure the sorted file makes sense before you remove the unsorted file\n</code></pre> <p>Generate pixel level image representing the factorization result</p> <pre><code># Make pixel level figures\ncmap=${output_path}/figure/${output_id}.rgb.tsv\ninput=${output_path}/${prefix}.pixel.sorted.tsv.gz\noutput=${figure_path}/${prefix}.pixel.png\n\n# plot pixel level images\nficture plot_pixel_full --input ${input} --color_table ${cmap} --output ${output} --plot_um_per_pixel 0.5 --full\n</code></pre> <p>Next, we can identify differentially expressed genes for each factor. This is a naive pseudo-bulk chi-squared test, please view the results with caution.</p> <pre><code># DE\nmax_pval_output=1e-3\nmin_fold_output=1.5\ninput=${output_path}/${prefix}.posterior.count.tsv.gz\noutput=${output_path}/${prefix}.bulk_chisq.tsv\nficture de_bulk --input ${input} --output ${output} --min_ct_per_feature ${min_ct_per_feature} --max_pval_output ${max_pval_output} --min_fold_output ${min_fold_output} --thread ${thread}\n\n\n# Report (color table and top DE genes)\ncmap=${output_path}/figure/${output_id}.rgb.tsv\noutput=${output_path}/${prefix}.factor.info.html\n\n# generate a report for each factor\nficture factor_report --path ${output_path} --pref ${prefix} --color_table ${cmap}\n</code></pre> <p>You may also want to generate heatmaps for individual factors. If the data is very large, making all individual factor maps may take some time.</p> <p>Generate everything in one run <pre><code># Make single factor heatmaps, plot_subbatch balances speed and memory ...\n# batch size 8 should be safe for 7 or 14G in most cases\noutput=${figure_path}/sub/${prefix}.pixel\nficture plot_pixel_single --input ${input} --output ${output} --plot_um_per_pixel 0.5 --full --all\n</code></pre></p> <p>Alternatively, you can generate by batch <pre><code>plot_subbatch=8\nst=0\ned=$((plot_subbatch+st-1))\nwhile [ ${st} -lt ${K} ]; do\n    if [ ${ed} -gt ${K} ]; then\n        ed=$((K-1))\n    fi\n    id_list=$( seq ${st} ${ed} )\n    echo $id_list\n\n    ficture plot_pixel_single --input ${input} --output ${output} --id_list ${id_list} --plot_um_per_pixel 0.5 --full\n    st=$((ed+1))\n    ed=$((plot_subbatch+st-1))\ndone\n</code></pre></p>"},{"location":"localrun/#output","title":"Output","text":"<p>In the above example the analysis outputs are stored in</p> <pre><code>${path}/analysis/${model_id} # examples/data/analysis/nF12.d_12\n</code></pre> <p>There is an html file reporting the color code and top genes of the inferred factors <pre><code>nF12.d_12.decode.prj_12.r_4_5.factor.info.html\n</code></pre></p> <p>Pixel level visualization is stored in <pre><code>figure/nF12.d_12.decode.prj_12.r_4_5.pixel.png\n</code></pre></p> <p>Pixel level output is stored in</p> <pre><code>nF12.d_12.decode.prj_12.r_4_5.pixel.sorted.tsv.gz\n</code></pre> <p>We store the top 3 factors and their corresponding posterior probabilities for each pixel in tab delimted text files. As a temporary hack for accessing specific regions in large dataset faster, we divided the data along one axis (X or Y), sorted within each block by the other axis. The first 3 lines of the file, starting with <code>##</code>, are metadata, the 4<sup>th</sup> line, starting with <code>#</code>, contains columns names. To use the file as plain text, you can ignore this complication and read the file from the 4<sup>th</sup> line.</p> <p>The first few lines of the file are as follows:</p> <pre><code>##K=12;TOPK=3\n##BLOCK_SIZE=2000;BLOCK_AXIS=X;INDEX_AXIS=Y\n##OFFSET_X=6690;OFFSET_Y=6772;SIZE_X=676;SIZE_Y=676;SCALE=100\n#BLOCK  X       Y       K1      K2      K3      P1      P2      P3\n0       10400   360     2       1       8       9.07e-01        9.27e-02        2.61e-13\n0       10669   360     2       1       8       9.36e-01        6.37e-02        4.20e-08\n0       10730   360     2       1       8       8.85e-01        1.15e-01        1.83e-05\n</code></pre> <p>The 4<sup>th</sup> line contains the column names. From the 5<sup>th</sup> line on, each line contains the information for one pixel with coordinates <code>(X, Y)</code>, the top 3 factors indicated by <code>K1, K2, K3</code> and their corresponding posterior probabilities <code>P1, P2, P3</code>. Factors are 0-indexed.</p> <p>The 1<sup>st</sup> line indicates that the data is from a model with 12 factors (<code>K=12</code>) and we store the top 3 factors for each pixel (<code>TOPK=3</code>).</p> <p>The 2<sup>nd</sup> line indicates that the data is separated into blocks by the X axis (<code>BLOCK_AXIS=X</code>) with block size 2000\\(\\mu m\\) (<code>BLOCK_SIZE=2000</code>), then within each block the data is sorted by the Y axis (<code>INDEX_AXIS=Y</code>). The block IDs (first column in the file) are integer multiples of the block size (in \\(\\mu m\\)), i.e. the 1<sup>st</sup> block, with \\(X \\in [0, 2000)\\) have block ID 0, the 2<sup>nd</sup> block, with \\(X \\in [2000, 4000)\\) have block ID 2000, etc.</p> <p>The 3<sup>rd</sup> line describes the translation between the stored cooredinates and the physical coordinates in \\(\\mu m\\). Take <code>(X, Y)</code> as a pixel coordinates read from the file, the physical coordinates in \\(\\mu m\\) is <code>(X / SCALE + OFFSET_X, Y / SCALE + OFFSET_Y)</code>. In this above example, the raw data from Vizgen MERSCOPE mouse liver data contains negative coordinates, but for convineince we shifted all coordinates to positive. <code>SIZE_X</code> and <code>SIZE_Y</code> record the size of the raw data in \\(\\mu m\\).</p>"},{"location":"localrun_legacy/","title":"Real data process in a local linux machine","text":"<p>We take a small sub-region of Vizgen MERSCOPE mouse liver data as an example.</p>"},{"location":"localrun_legacy/#setup","title":"Setup","text":""},{"location":"localrun_legacy/#input","title":"Input","text":"<p><pre><code>examples/data/transcripts.tsv.gz\nexamples/data/feature.clean.tsv.gz\n</code></pre> (All filese are tab-delimited text files unless specified otherwise.)</p> <p>See the following explanation for each file. If you have trouble, try Format input to see some examples of formating raw output from different platforms.</p> <p>Transcripts</p> <p>One file contains the molecular or pixel level information, the required columns are <code>X</code>, <code>Y</code>, <code>gene</code>, and <code>Count</code>. (There could be other columns in the file which would be ignored.)</p> <p>The coordinates <code>(X, Y)</code> can be float or integer numbers in arbitrary units, but if it is not in the unit of \\(\\mu m\\) we would need to specify the translation ratio later.</p> <p>The file has to be sorted by one of the coordinates. (Usually it is the longer axis, but it does not matter if the tissue area is not super asymmetric.)</p> <p><code>Count</code> (could be any other name) is the number of transcripts for the specified <code>gene</code> observed at the coordinate. For imaging based technologies where each molecule has its unique coordinates, <code>Count</code> could be always 1.</p> <p>Gene list</p> <p>Another file contains the (unique) names of genes that should be used in analysis. The required columns is just <code>gene</code> (including the header), the naming of genes should match the <code>gene</code> column in the transcript file. If your data contain negative control probes or if you would like to remove certain genes this is where you can specify. (If you would like to use all genes present in your input transcript file the gene list is not necessary, but you would need to modify the command in <code>generic_III.sh</code> to remove the argument <code>--feature</code> )</p> <p>Meta data</p> <p>We also prefer to keep a file listing the min and max of the coordinates (this is primarily for visuaizing very big tissue region where we do not read all data at once but would want to know the image dimension). The unit of the coordinates is micrometer. <pre><code>examples/data/coordinate_minmax.tsv\n</code></pre></p>"},{"location":"localrun_legacy/#prepare-environment","title":"Prepare environment","text":"<p>Suppose you have installed dependencies following Install. This document assume you are on the <code>stable</code> branch.</p> <p>Activate your virtual environmnet if needed <pre><code>source venv/with/requirements/installed/bin/activate\n</code></pre></p>"},{"location":"localrun_legacy/#process","title":"Process","text":"<p>Specify the base directory that contains the input data <pre><code>path=examples/data\n</code></pre></p> <p>Data specific setup:</p> <p><code>mu_scale</code> is the ratio between \\(\\mu m\\) and the unit used in the transcript coordinates. For example, if the coordinates are sotred in <code>nm</code> this number should be <code>1000</code>.</p> <p><code>key</code> is the column name in the transcripts file corresponding to the gene counts (<code>Count</code> in our example). <code>major_axis</code> specify which axis the transcript file is sorted by.</p> <pre><code>mu_scale=1 # If your data's coordinates are already in micrometer\nkey=Count\nmajor_axis=Y # If your data is sorted by the Y-axis\ngitpath=path/to/ficture # path to this repository\n</code></pre>"},{"location":"localrun_legacy/#preprocessing","title":"Preprocessing","text":"<p>Create pixel minibatches (<code>${path}/batched.matrix.tsv.gz</code>) <pre><code>batch_size=500\nbatch_buff=30\ninput=${path}/transcripts.tsv.gz\noutput=${path}/batched.matrix.tsv.gz\nbatch=${path}/batched.matrix.tsv\n\npython ${gitpath}/script/make_spatial_minibatch.py --input ${input} --output ${batch} --mu_scale ${mu_scale} --batch_size ${batch_size} --batch_buff ${batch_buff} --major_axis ${major_axis}\n\nsort -S 4G -k2,2n -k1,1g ${batch} | gzip -c &gt; ${batch}.gz\nrm ${batch}\n</code></pre></p> <p>Prepare training minibatches, only need to run once if you plan to fit multiple models (say with different number of factors) <pre><code># Prepare training minibatches, only need to run once if you plan to fit multiple models (say with different number of factors)\ntrain_width=12 # \\sqrt{3} x the side length of the hexagon (um)\nmin_ct_per_unit=50\ninput=${path}/transcripts.tsv.gz\nout=${path}/hexagon.d_${train_width}.tsv\npython ${gitpath}/script/make_dge_univ.py --key ${key} --count_header ${key} --input ${input} --output ${out} --hex_width ${train_width} --n_move 2 --min_ct_per_unit ${min_ct_per_unit} --mu_scale ${mu_scale} --precision 2 --major_axis ${major_axis}\n\nsort -S 4G -k1,1n ${out} | gzip -c &gt; ${out}.gz # Shuffle hexagons\nrm ${out}\n</code></pre></p>"},{"location":"localrun_legacy/#model-training","title":"Model training","text":"<p>Parameters for initializing the model <pre><code>nFactor=12 # Number of factors\nsliding_step=2\ntrain_nEpoch=3\n# train_width=12 # should be the same as used in the above step\nmodel_id=nF${nFactor}.d_${train_width} # An identifier kept in output file names\nmin_ct_per_feature=20 # Ignore genes with total count \\&lt; 20\nR=10 # We use R random initializations and pick one to fit the full model\nthread=4 # Number of threads to use\n</code></pre></p> <p>Initialize the model <pre><code># parameters\nmin_ct_per_unit_fit=20\ncmap_name=\"turbo\"\nfit_nmove=$((fit_width/anchor_res))\n\n# output identifiers\nmodel_id=nF${nFactor}.d_${train_width}\noutput_id=${model_id}\nanchor_info=prj_${fit_width}.r_${anchor_res}\noutput_path=${path}/analysis/${model_id}\nfigure_path=${output_path}/figure\nif [ ! -d \"${figure_path}/sub\" ]; then\n    mkdir -p ${figure_path}/sub\nfi\n\n\n# input files\nhexagon=${path}/hexagon.d_${train_width}.tsv.gz\npixel=${path}/transcripts.tsv.gz\nfeature=${path}/feature.clean.tsv.gz\n# output\noutput=${output_path}/${output_id}\nmodel=${output}.model.p\n\n# Fit model\npython ${gitpath}/script/init_model_selection.py --input ${hexagon} --output ${output} --feature ${feature} --nFactor ${nFactor} --epoch ${train_nEpoch} --epoch_id_length 2 --unit_attr X Y --key ${key} --min_ct_per_feature ${min_ct_per_feature} --test_split 0.5 --R ${R} --thread ${thread}\n\n# Choose color\ninput=${output_path}/${output_id}.fit_result.tsv.gz\noutput=${figure_path}/${output_id}\ncmap=${figure_path}/${output_id}.rgb.tsv\npython ${gitpath}/script/choose_color.py --input ${input} --output ${output} --cmap_name ${cmap_name}\n\n# Coarse plot for inspection\ncmap=${figure_path}/${output_id}.rgb.tsv\ninput=${output_path}/${output_id}.fit_result.tsv.gz\noutput=${figure_path}/${output_id}.coarse\nfillr=$((fit_width/2+1))\npython ${gitpath}/script/plot_base.py --input ${input} --output ${output} --fill_range ${fillr} --color_table ${cmap} --plot_um_per_pixel 1 --plot_discretized\n</code></pre></p>"},{"location":"localrun_legacy/#pixel-level-decoding","title":"Pixel level decoding","text":"<p>Parameters for pixel level decoding <pre><code>fit_width=12 # Often equal or smaller than train_width (um)\nanchor_res=4 # Distance between adjacent anchor points (um)\nradius=$(($anchor_res+1))\nanchor_info=prj_${fit_width}.r_${anchor_res} # An identifier\ncoor=${path}/coordinate_minmax.tsv\ncmap=${figure_path}/${output_id}.rgb.tsv\n</code></pre></p> <pre><code># Transform\noutput=${output_path}/${output_id}.${anchor_info}\npython ${gitpath}/script/transform_univ.py --input ${pixel} --output_pref ${output} --model ${model} --key ${key} --major_axis ${major_axis} --hex_width ${fit_width} --n_move ${fit_nmove} --min_ct_per_unit ${min_ct_per_unit_fit} --mu_scale ${mu_scale} --thread ${thread} --precision 2\n\n# Pixel level decoding &amp; visualization\nprefix=${output_id}.decode.${anchor_info}_${radius}\ninput=${path}/batched.matrix.tsv.gz\nanchor=${output_path}/${output_id}.${anchor_info}.fit_result.tsv.gz\noutput=${output_path}/${prefix}\n# Output only a few top factors per pixel\ntopk=3 # Fix for now\npython ${gitpath}/script/slda_decode.py --input ${input} --output ${output} --model ${model} --anchor ${anchor} --anchor_in_um --neighbor_radius ${radius} --mu_scale ${mu_scale} --key ${key} --precision 0.1 --lite_topk_output_pixel ${topk} --lite_topk_output_anchor ${topk} --thread ${thread}\n</code></pre>"},{"location":"localrun_legacy/#optional-post-processing","title":"Optional post-processing","text":"<p>The following is not strictly necessary but it generates summary statistics and helps visualization.</p> <p>Sort the pixel level output, this is for visualize large images with limited memory usage. <pre><code>input=${output_path}/${prefix}.pixel.tsv.gz # j, X, Y, K1, ..., KJ, P1, ..., PJ, J=topk\noutput=${output_path}/${prefix}.pixel.sorted.tsv.gz\n\nK=$( echo $model_id | sed 's/nF\\([0-9]\\{1,\\}\\)\\..*/\\1/' )\nwhile IFS=$'\\t' read -r r_key r_val; do\n    export \"${r_key}\"=\"${r_val}\"\ndone &lt; ${coor}\necho -e \"${xmin}, ${xmax}; ${ymin}, ${ymax}\"\n\noffsetx=${xmin}\noffsety=${ymin}\nrangex=$( echo \"(${xmax} - ${xmin} + 0.5)/1+1\" | bc )\nrangey=$( echo \"(${ymax} - ${ymin} + 0.5)/1+1\" | bc )\nbsize=2000\nscale=100\nheader=\"##K=${K};TOPK=3\\n##BLOCK_SIZE=${bsize};BLOCK_AXIS=X;INDEX_AXIS=Y\\n##OFFSET_X=${offsetx};OFFSET_Y=${offsety};SIZE_X=${rangex};SIZE_Y=${rangey};SCALE=${scale}\\n#BLOCK\\tX\\tY\\tK1\\tK2\\tK3\\tP1\\tP2\\tP3\"\n\n(echo -e \"${header}\" &amp;&amp; zcat ${input} | tail -n +2 | perl -slane '$F[0]=int(($F[1]-$offx)/$bsize) * $bsize; $F[1]=int(($F[1]-$offx)*$scale); $F[1]=($F[1]&gt;=0)?$F[1]:0; $F[2]=int(($F[2]-$offy)*$scale); $F[2]=($F[2]&gt;=0)?$F[2]:0; print join(\"\\t\", @F);' -- -bsize=${bsize} -scale=${scale} -offx=${offsetx} -offy=${offsety} | sort -S 4G -k1,1g -k3,3g ) | bgzip -c &gt; ${output}\n\ntabix -f -s1 -b3 -e3 ${output}\nrm ${input}\n</code></pre></p> <p>Report differentially expressed genes. This is a naive pseudo-bulk chi-squared test, please view the results with caution. <pre><code># DE\nmax_pval_output=1e-3\nmin_fold_output=1.5\ninput=${output_path}/${prefix}.posterior.count.tsv.gz\noutput=${output_path}/${prefix}.bulk_chisq.tsv\npython ${gitpath}/script/de_bulk.py --input ${input} --output ${output} --min_ct_per_feature ${min_ct_per_feature} --max_pval_output ${max_pval_output} --min_fold_output ${min_fold_output} --thread ${thread}\n\n\n# Report (color table and top DE genes)\ncmap=${output_path}/figure/${output_id}.rgb.tsv\noutput=${output_path}/${prefix}.factor.info.html\npython ${gitpath}/script/factor_report.py --path ${output_path} --pref ${prefix} --color_table ${cmap}\n</code></pre></p> <p>Generalize pixel level images representing the factorization result <pre><code># Make pixel level figures\ncmap=${output_path}/figure/${output_id}.rgb.tsv\ninput=${output_path}/${prefix}.pixel.sorted.tsv.gz\noutput=${figure_path}/${prefix}.pixel.png\npython ${gitpath}/script/plot_pixel_full.py --input ${input} --color_table ${cmap} --output ${output} --plot_um_per_pixel 0.5 --full\n</code></pre></p> <p>Generate heatmaps for individual factors. If the data is very large, making all individual factor maps may take some time.</p> <p>Generate everything in one run <pre><code># Make single factor heatmaps, plot_subbatch balances speed and memory ...\n# batch size 8 should be safe for 7 or 14G in most cases\noutput=${figure_path}/sub/${prefix}.pixel\npython ${gitpath}/script/plot_pixel_single.py --input ${input} --output ${output} --plot_um_per_pixel 0.5 --full --all\n</code></pre></p> <p>Alternatively, you can generate by batch <pre><code>plot_subbatch=8\nst=0\ned=$((plot_subbatch+st-1))\nwhile [ ${st} -lt ${K} ]; do\n    if [ ${ed} -gt ${K} ]; then\n        ed=$((K-1))\n    fi\n    id_list=$( seq ${st} ${ed} )\n    echo $id_list\n\n    python ${gitpath}/script/plot_pixel_single.py --input ${input} --output ${output} --id_list ${id_list} --plot_um_per_pixel 0.5 --full\n    st=$((ed+1))\n    ed=$((plot_subbatch+st-1))\ndone\n</code></pre></p>"},{"location":"localrun_legacy/#output","title":"Output","text":"<p>In the above example the analysis outputs are stored in <pre><code>${path}/analysis/${model_id} # examples/data/analysis/nF12.d_12\n</code></pre></p> <p>There is an html file reporting the color code and top genes of the inferred factors <pre><code>nF12.d_12.decode.prj_12.r_4_5.factor.info.html\n</code></pre></p> <p>Pixel level visualizating <pre><code>figure/nF12.d_12.decode.prj_12.r_4_5.pixel.png\n</code></pre></p> <p>Pixel level output is <pre><code>nF12.d_12.decode.prj_12.r_4_5.pixel.sorted.tsv.gz\n</code></pre></p> <p>We store the top 3 factors and their corresponding posterior probabilities for each pixel in tab delimted text files. As a temporary hack for accessing specific regions in large dataset faster, we divided the data along one axis (X or Y), sorted within each block by the other axis. The first 3 lines of the file, starting with <code>##</code>, are metadata, the 4<sup>th</sup> line, starting with <code>#</code>, contains columns names. To use the file as plain text, you can ignore this complication and read the file from the 4<sup>th</sup> line.</p> <p>The first few lines of the file are as follows:</p> <pre><code>##K=12;TOPK=3\n##BLOCK_SIZE=2000;BLOCK_AXIS=X;INDEX_AXIS=Y\n##OFFSET_X=6690;OFFSET_Y=6772;SIZE_X=676;SIZE_Y=676;SCALE=100\n#BLOCK  X       Y       K1      K2      K3      P1      P2      P3\n0       10400   360     2       1       8       9.07e-01        9.27e-02        2.61e-13\n0       10669   360     2       1       8       9.36e-01        6.37e-02        4.20e-08\n0       10730   360     2       1       8       8.85e-01        1.15e-01        1.83e-05\n</code></pre> <p>The 4<sup>th</sup> line contains the column names. From the 5<sup>th</sup> line on, each line contains the information for one pixel with coordinates <code>(X, Y)</code>, the top 3 factors indicated by <code>K1, K2, K3</code> and their corresponding posterior probabilities <code>P1, P2, P3</code>. Factors are 0-indexed.</p> <p>The 1<sup>st</sup> line indicates that the data is from a model with 12 factors (<code>K=12</code>) and we store the top 3 factors for each pixel (<code>TOPK=3</code>).</p> <p>The 2<sup>nd</sup> line indicates that the data is separated into blocks by the X axis (<code>BLOCK_AXIS=X</code>) with block size 2000\\(\\mu m\\) (<code>BLOCK_SIZE=2000</code>), then within each block the data is sorted by the Y axis (<code>INDEX_AXIS=Y</code>). The block IDs (first column in the file) are integer multiples of the block size (in \\(\\mu m\\)), i.e. the 1<sup>st</sup> block, with \\(X \\in [0, 2000)\\) have block ID 0, the 2<sup>nd</sup> block, with \\(X \\in [2000, 4000)\\) have block ID 2000, etc.</p> <p>The 3<sup>rd</sup> line describes the translation between the stored cooredinates and the physical coordinates in \\(\\mu m\\). Take <code>(X, Y)</code> as a pixel coordinates read from the file, the physical coordinates in \\(\\mu m\\) is <code>(X / SCALE + OFFSET_X, Y / SCALE + OFFSET_Y)</code>. In this above example, the raw data from Vizgen MERSCOPE mouse liver data contains negative coordinates, but for convineince we shifted all coordinates to positive. <code>SIZE_X</code> and <code>SIZE_Y</code> record the size of the raw data in \\(\\mu m\\).</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>The following is a quickstart guide to get you started with FICTURE.</p>"},{"location":"quickstart/#install-ficture","title":"Install FICTURE","text":"<p>The simplest way to FICTURE using PyPI. Please see installing FICTURE for other options.</p> <pre><code>## Install FICTURE from PyPI\npip install ficture\n</code></pre>"},{"location":"quickstart/#clone-the-repository","title":"Clone the repository","text":"<p>To access the example data, let's clone the FICTURE repository using the following command:</p> <pre><code>## Clone the FICTURE repository to access the example data\ngit clone https://github.com/seqscope/ficture\n</code></pre>"},{"location":"quickstart/#run-example-datasets","title":"Run example datasets","text":"<p>Using example datasets, you can run FICTURE with the following command:</p> <pre><code>## Run all steps together with the example datasets\nficture run_together --in-tsv examples/data/transcripts.tsv.gz \\\n    --in-minmax examples/data/coordinate_minmax.tsv \\\n    --in-feature examples/data/feature.clean.tsv.gz \\\n    --major-axis Y \\\n    --out-dir output1 --all\n</code></pre> <p>This command will create a GNU makefile and run the FICTURE local pipeline.</p> <p>This will run FICTURE on the example datasets and save the results in the <code>output1</code> directory.</p>"},{"location":"quickstart/#running-ficture-with-multiple-parameter-settings","title":"Running FICTURE with multiple parameter settings","text":"<p>You can change the parameter settings, such as the width of training parameters, or the number of concurrent jobs to run.</p> <pre><code>## Specify multiple LDA training widths and number of factors\nficture run_together --in-tsv examples/data/transcripts.tsv.gz \\\n    --in-minmax examples/data/coordinate_minmax.tsv \\\n    --in-feature examples/data/feature.clean.tsv.gz \\\n    --major-axis Y \\\n    --out-dir output2 \\\n    --train-width 12,18 \\\n    --n-factor 6,12 \\\n    --n-jobs 4 \\\n    --plot-each-factor \\\n    --all\n</code></pre>"},{"location":"quickstart/#running-ficture-with-your-own-model-matrix-and-color-scheme","title":"Running FICTURE with your own model matrix and color scheme","text":"<p>Caution: your input model and color scheme file must be in the same format as those created by the above commands. <pre><code>ficture run_together --in-tsv ../examples/data/transcripts.tsv.gz \\\n    --in-minmax ../examples/data/coordinate_minmax.tsv \\\n    --in-feature ../examples/data/feature.clean.tsv.gz \\\n    --out-dir ./out --major-axis Y --threads 4 \\\n    --decode-from-external-model \\\n    --fit-width 12 \\\n    --external-model YOUR_MODEL_FILE \\\n    --external-cmap YOUR_COLOR_SCHEME_FILE\n</code></pre></p>"},{"location":"quickstart/#more-information","title":"More information","text":"<p>If you want to see more options with the local pipeline, please run the following command:</p> <pre><code>ficture run_together --help\n</code></pre> <p>Note that the local pipeline is a wrapper for individual commands, and provided for convenience. For large datasets, you may need to run individual commands to mitigate memory or CPU constraints.</p> <p>If you want to run FICTURE with individual commands rather than using the local pipeline, please refer to other sections in this documentation.</p>"},{"location":"run/","title":"Processing large real datasets","text":"<p>This document describes how to run large real datasets with FICTURE. Most of the contents here are similar to the small run document, but we provide more details on how to submit jobs to a SLURM cluster, using prepared scripts in the <code>examples/script/</code> directory.</p> <p>In this example, we will use a small sub-region of Vizgen MERSCOPE mouse liver data as an example. (The same region we showed in supplementary figure X)</p> <p>This document assume you have intalled FICTURE. See installing FICTURE for more details.</p>"},{"location":"run/#input","title":"Input","text":"<p><pre><code>examples/data/transcripts.tsv.gz\n</code></pre> (All filese are tab-delimited text files unless specified otherwise.)</p> <p>Transcripts</p> <p>One file contains the molecular or pixel level information, the required columns are <code>X</code>, <code>Y</code>, <code>gene</code>, and <code>Count</code>. (There could be other columns in the file which would be ignored.)</p> <p>The coordinates <code>(X, Y)</code> can be float or integer numbers in arbitrary units, but if it is not in the unit of \\(\\mu m\\) we would need to specify the translation ratio later.</p> <p>The file has to be sorted by one of the coordinates. (Usually it is the longer axis, but it does not matter if the tissue area is not super asymmetric.)</p> <p><code>Count</code> (could be any other name) is the number of transcripts for the specified <code>gene</code> observed at the coordinate. For imaging based technologies where each molecule has its unique coordinates, <code>Count</code> could be always 1.</p> <p>Bounding box of spatial coordinates</p> <p>We also prefer to keep a file listing the min and max of the coordinates (this is primarily for visualizing very big tissue region where we do not read all data at once but would want to know the image dimension). The unit of the coordinates is micrometer. <pre><code>examples/data/coordinate_minmax.tsv\n</code></pre></p>"},{"location":"run/#process","title":"Process","text":"<p>Specify the base directory that contains the input data <pre><code>path=examples/data\n</code></pre></p> <p>Data specific setup:</p> <p><code>mu_scale</code> is the ratio between \\(\\mu m\\) and the unit used in the transcript coordinates. For example, if the coordinates are sotred in <code>nm</code> this number should be <code>1000</code>.</p> <p><code>key</code> is the column name in the transcripts file corresponding to the gene counts (<code>Count</code> in our example). <code>MJ</code> specify which axis the transcript file is sorted by.</p> <pre><code>mu_scale=1 # If your data's coordinates are already in micrometer\nkey=Count\nMJ=Y # If your data is sorted by the Y-axis\nenv=venv/with/ficture/installed/bin/activate\n\n# Uncomment and modify the following line if you are using SLURM\n#SLURM_ACCOUNT= # For submitting jobs to slurm\n</code></pre> <p>Example bash scripts are in <code>examples/script/</code>, you will need to modify them to work on your system.</p> <p>Create pixel minibatches (<code>${path}/batched.matrix.tsv.gz</code>)</p> <pre><code>input=${path}/transcripts.tsv.gz\noutput=${path}/batched.matrix.tsv.gz\nrec=$(sbatch --job-name=vz1 --account=${SLURM_ACCOUNT} --partition=standard --cpus-per-task=1 examples/script/generic_I.sh input=${input} output=${output} MJ=${MJ} env=${env} )\nIFS=' ' read -ra ADDR &lt;&lt;&lt; \"$rec\"\njobid1=${ADDR[3]}\n</code></pre> <p>Set up parameters for initializing the model. <pre><code>nFactor=12 # Number of factors\nsliding_step=2\ntrain_nEpoch=3\ntrain_width=12 # \\sqrt{3} x the side length of the hexagon (um)\nmodel_id=nF${nFactor}.d_${train_width} # An identifier kept in output file names\nmin_ct_per_feature=20 # Ignore genes with total count \\&lt; 20\nR=10 # We use R random initializations and pick one to fit the full model\nthread=4 # Number of threads to use\n</code></pre></p> <p>Parameters for pixel level decoding <pre><code>fit_width=12 # Often equal or smaller than train_width (um)\nanchor_res=4 # Distance between adjacent anchor points (um)\nradius=$(($anchor_res+1))\nanchor_info=prj_${fit_width}.r_${anchor_res} # An identifier\ncoor=${path}/coordinate_minmax.tsv\n</code></pre></p> <p>Perform model fitting and pixel-level decoding</p> <pre><code># Prepare training minibatches, only need to run once if you plan to fit multiple models (say with different number of factors)\ninput=${path}/transcripts.tsv.gz\nhexagon=${path}/hexagon.d_${train_width}.tsv.gz\nrec=$(sbatch --job-name=vz2 --account=${SLURM_ACCOUNT} --partition=standard --cpus-per-task=1 examples/script/generic_II.sh env=${env} key=${key} mu_scale=${mu_scale} major_axis=${MJ} path=${path} input=${input} output=${hexagon} width=${train_width} sliding_step=${sliding_step})\nIFS=' ' read -ra ADDR &lt;&lt;&lt; \"$rec\"\njobid2=${ADDR[3]}\n\n# Model training\nrec=$(sbatch --job-name=vz3 --account=${SLURM_ACCOUNT} --partition=standard --cpus-per-task=${thread} --dependency=afterok:${jobid2} examples/script/generic_III.sh env=${env} key=${key} mu_scale=${mu_scale} major_axis=${MJ} path=${path} pixel=${input} hexagon=${hexagon} model_id=${model_id} train_width=${train_width} nFactor=${nFactor} R=${R} train_nEpoch=${train_nEpoch} fit_width=${fit_width} anchor_res=${anchor_res} min_ct_per_feature=${min_ct_per_feature} thread=${thread})\nIFS=' ' read -ra ADDR &lt;&lt;&lt; \"$rec\"\njobid3=${ADDR[3]}\n\n# Pixel level decoding &amp; visualization\nrec=$(sbatch --job-name=vz4 --account=${SLURM_ACCOUNT} --partition=standard --cpus-per-task=${thread} --dependency=afterok:${jobid3},${jobid1} examples/script/generic_V.sh env=${env} key=${key} mu_scale=${mu_scale} path=${path} model_id=${model_id} anchor_info=${anchor_info} radius=${radius} coor=${coor} thread=${thread})\nIFS=' ' read -ra ADDR &lt;&lt;&lt; \"$rec\"\njobid4=${ADDR[3]}\n</code></pre>"},{"location":"run/#output","title":"Output","text":"<p>In the above example the analysis outputs are stored in</p> <pre><code>${path}/analysis/${model_id} # examples/data/analysis/nF12.d_12\n</code></pre> <p>There is an html file reporting the color code and top genes of the inferred factors</p> <pre><code>nF12.d_12.decode.prj_12.r_4_5.factor.info.html\n</code></pre> <p>Pixel level visualizating</p> <pre><code>figure/nF12.d_12.decode.prj_12.r_4_5.pixel.png\n</code></pre> <p>Pixel level output is</p> <pre><code>nF12.d_12.decode.prj_12.r_4_5.pixel.sorted.tsv.gz\n</code></pre> <p>We store the top 3 factors and their corresponding posterior probabilities for each pixel in tab delimted text files. As a temporary hack for accessing specific regions in large dataset faster, we divided the data along one axis (X or Y), sorted within each block by the other axis. The first 3 lines of the file, starting with <code>##</code>, are metadata, the 4<sup>th</sup> line, starting with <code>#</code>, contains columns names. To use the file as plain text, you can ignore this complication and read the file from the 4<sup>th</sup> line.</p> <p>The first few lines of the file are as follows:</p> <pre><code>##K=12;TOPK=3\n##BLOCK_SIZE=2000;BLOCK_AXIS=X;INDEX_AXIS=Y\n##OFFSET_X=6690;OFFSET_Y=6772;SIZE_X=676;SIZE_Y=676;SCALE=100\n#BLOCK  X       Y       K1      K2      K3      P1      P2      P3\n0       10400   360     2       1       8       9.07e-01        9.27e-02        2.61e-13\n0       10669   360     2       1       8       9.36e-01        6.37e-02        4.20e-08\n0       10730   360     2       1       8       8.85e-01        1.15e-01        1.83e-05\n</code></pre> <p>The 4<sup>th</sup> line contains the column names. From the 5<sup>th</sup> line on, each line contains the information for one pixel with coordinates <code>(X, Y)</code>, the top 3 factors indicated by <code>K1, K2, K3</code> and their corresponding posterior probabilities <code>P1, P2, P3</code>. Factors are 0-indexed.</p> <p>The 1<sup>st</sup> line indicates that the data is from a model with 12 factors (<code>K=12</code>) and we store the top 3 factors for each pixel (<code>TOPK=3</code>).</p> <p>The 2<sup>nd</sup> line indicates that the data is separated into blocks by the X axis (<code>BLOCK_AXIS=X</code>) with block size 2000\\(\\mu m\\) (<code>BLOCK_SIZE=2000</code>), then within each block the data is sorted by the Y axis (<code>INDEX_AXIS=Y</code>). The block IDs (first column in the file) are integer multiples of the block size (in \\(\\mu m\\)), i.e. the 1<sup>st</sup> block, with \\(X \\in [0, 2000)\\) have block ID 0, the 2<sup>nd</sup> block, with \\(X \\in [2000, 4000)\\) have block ID 2000, etc.</p> <p>The 3<sup>rd</sup> line describes the translation between the stored cooredinates and the physical coordinates in \\(\\mu m\\). Take <code>(X, Y)</code> as a pixel coordinates read from the file, the physical coordinates in \\(\\mu m\\) is <code>(X / SCALE + OFFSET_X, Y / SCALE + OFFSET_Y)</code>. In this above example, the raw data from Vizgen MERSCOPE mouse liver data contains negative coordinates, but for convineince we shifted all coordinates to positive. <code>SIZE_X</code> and <code>SIZE_Y</code> record the size of the raw data in \\(\\mu m\\).</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"format_input/cosmx/","title":"Process CosMx SMI raw data","text":"<p>Locate the transcript file from your SMI output, most likely it is named <code>*_tx_file.csv.gz</code> with the following columns</p> <pre><code>\"fov\",\"cell_ID\",\"x_global_px\",\"y_global_px\",\"x_local_px\",\"y_local_px\",\"z\",\"target\",\"CellComp\"\n1,0,298943.990047619,19493.2809095238,896.371,4433.7571,0,\"Snap25\",\"None\"\n1,0,298685.619047619,19489.3238095238,638,4429.8,0,\"Fth1\",\"None\"\n1,0,298688.648047619,19487.8095095238,641.029,4428.2857,0,\"Dnm1\",\"None\"\n1,0,298943.890047619,19478.7667095238,896.271,4419.2429,0,\"Pbx1\",\"Nuclear\"\n</code></pre> <p>What we need are <code>x_global_px</code>, <code>y_global_px</code>, and <code>target</code>. Optionally, we could carry over some pixel level annotations to the output file by specifying <code>--annotation</code>, see the following example.</p> <p>We may also like to translate the pixel unit into micrometer, the ratio can be found in a SMI Data File ReadMe come with your raw data. For example, for the public mouse brain dataset the README says</p> <ul> <li>x_local_px<ul> <li>The x position of this transcript within the FOV, measured in pixels. To convert to microns multiply the pixel value by 0.168 um per pixel.</li> </ul> </li> </ul> <p>So in the following commands we set <code>px_to_um=0.168</code>, then for all later analysis we would use <code>mu_scale=1</code> The output is sorted first along the Y-axis, so later you would set <code>major_axis=Y</code>.</p> <p>(Alternatively, we can preferve the integer pixel coordinates for now by not setting <code>--px_to_um</code> (or set it to 1) then specify <code>mu_scale=5.95</code> (1/0.168) when we later process the data and run FICTURE.)</p> <p>The python script can be found in <code>ficture/misc</code>. use <code>python format_cosmx.py -h</code> to see the full options.</p> <pre><code>input=/path/to/input/Tissue5_tx_file.csv.gz # Change it to your transcript file\npath=/path/to/output\niden=brain # how you identify your files\ndummy=\"Negativ|System\" # Name or regex that match the negative control probs\npx_to_um=0.168 # convert the pixel unit in the input to micrometer\n\noutput=${path}/filtered.matrix.${iden}.tsv\nfeature=${path}/feature.clean.${iden}.tsv.gz\n\npython misc/format_cosmx.py --input ${input} --output ${output} --feature ${feature} --dummy_genes ${dummy} --px_to_um ${px_to_um} --annotation cell_ID --precision 2\nsort -k2,2g -k1,1g ${output} | gzip -c &gt; ${output}.gz\nrm ${output}\n</code></pre> <p>If we would like to merge pixels with (almost?) identical coordinates, replace the last two lines by <pre><code>sort -k2,2g -k1,1g ${output} |\nawk 'BEGIN { OFS=\"\\t\"; print \"X\", \"Y\", \"gene\", \"cell_ID\", \"Count\" }\n     NR &gt; 1 {\n       if ($1 == prevX &amp;&amp; $2 == prevY &amp;&amp; $3 == prevGene) {\n         sumCount += $5;\n       } else {\n         if (NR &gt; 2) {\n           print prevX, prevY, prevGene, firstCellID, sumCount;\n         }\n         prevX = $1; prevY = $2; prevGene = $3; firstCellID = $4; sumCount = $5;\n       }\n     }\n     END { print prevX, prevY, prevGene, firstCellID, sumCount; }' | gzip -c &gt; ${output}.gz\n\nrm ${output}\n</code></pre> You might need to asjust the column numbers depending on what annotation columns you have chosen to retain in the output.</p> <p>(If your data is very dense it may be nicer to collapse, but it would not affect the analysis much.)</p>"},{"location":"format_input/visiumHD/","title":"Process Visium HD raw data","text":""},{"location":"format_input/visiumHD/#key-points","title":"Key points","text":"<p>1) Look at the json file, find 'microns_per_pixel' and later set <code>mu_scale=1/microns_per_pixel</code> for your analysis. 2) Combine information from multiple raw files and create a single input file sorted by one axis, either X or Y. 3) The coordinates in the input files are in \"pixel\" unit, but please keep track of the min and max coordinate values of X and Y axis in micrometer, they would be needed to create the final pixel level visualization. 4) Since Visium HD has resolution \\(2\\mu m\\), set <code>--plot_um_per_pixel 2</code> when visualizing the final pixel output (in <code>ficture plot_pixel_full</code>).</p>"},{"location":"format_input/visiumHD/#alternative-using-spatula-convert-sge-command","title":"Alternative: Using <code>spatula convert-sge</code> command","text":"<p>The spatula sge-convert tool offers a convenient way to convert Visium HD raw data to FICTURE input format. You may want to use the tool to convert the raw Visium HD data to FICTURE input format instead of following the manual steps below.</p>"},{"location":"format_input/visiumHD/#details","title":"Details","text":"<p>Visium HD output contains a sparse count matrix and a separate parquet file defining pixels' spatial locations.</p> <p>Locate Visium HD outputs <pre><code>brc_parq=/path/to/spatial/tissue_positions.parquet\nmtx_path=/path/to/filtered_feature_bc_matrix # or raw_feature_bc_matrix\nopath=/output/directory\n</code></pre></p> <p>The barcode (<code>tissue_positions.parquet</code>) file looks like <pre><code>barcode     in_tissue   array_row   array_col   pxl_row_in_fullres  pxl_col_in_fullres\ns_002um_00000_00000-1   0   0   0   44.189527   21030.743288\ns_002um_00000_00001-1   0   0   1   44.302100   21023.440349\n</code></pre> (but stored in the parquet format)</p> <p><code>pxl_row_in_fullres</code> and <code>pxl_col_in_fullres</code> are in the unit of \"pixel\", we need to look at the <code>scalefactors_json.json</code> file (should be in the same folder as <code>tissue_positions.parquet</code>) to get its ratio to micrometer. In our example the json file looks like</p> <p><pre><code>{\n    \"spot_diameter_fullres\": 7.303953797779634,\n    \"bin_size_um\": 2.0,\n    \"microns_per_pixel\": 0.2738242950835738,\n    \"regist_target_img_scalef\": 0.2505533,\n    \"tissue_lowres_scalef\": 0.02505533,\n    \"fiducial_diameter_fullres\": 1205.1523766336395,\n    \"tissue_hires_scalef\": 0.2505533\n}\n</code></pre> Record the number <code>microns_per_pixel</code> and later set <code>mu_scale=1/microns_per_pixel</code> for your analysis.</p> <p>The matrix directory looks like <pre><code>ls ${mtx_path}\n</code></pre></p> <pre><code>barcodes.tsv.gz  features.tsv.gz  matrix.mtx.gz\n</code></pre> <p>We need to annotate the sparse matrix <code>matrix.mtx.gz</code> with barcode locations from <code>tissue_positions.parquet</code> by getting the barcode ID from <code>barcodes.tsv.gz</code> then lookuping its spatial coordinate. Unfortunately barcodes in <code>tissue_positions.parquet</code> and in <code>barcodes.tsv.gz</code> are stored in different orders and the parquet file contains a single row group (why??) based on the few public datasets we've inspected, making this process uglier. Although one could read all four files fully in memory and match them, the following is a slower alternative.</p> <p>The requirements for the merged file are</p> <p>1) Containing the following columns: X, Y, gene, Count.</p> <p>2) Is sorted according to one axis. The output from the following commands is sorted first along the Y-axis, so later you would set <code>major_axis=Y</code>.</p> <p>Given the current data formats, we first match barcodes' integer indices in the matrix with their spatial locations, then annotate the spatial locations and gene IDs to the sparse count matrix.</p> <p>You may need to install a tool to read parquet file, one option is <code>pip install parquet-tools</code>. The following command takes ~8.5 min for the public Visium HD mouse brain dataset.</p> <pre><code>bfile=${mtx_path}/barcodes.tsv.gz\nmfile=${mtx_path}/matrix.mtx.gz\nffile=${mtx_path}/features.tsv.gz\noutput=${opath}/transcripts.tsv.gz\n\n# Extract the spatial locations\nbrc_raw=${opath}/tissue_positions.raw.csv\nparquet-tools csv ${brc_parq} &gt; ${brc_raw}\n\n# Check coordinate range (for future record)\nmicrons_per_pixel=0.2738242950835738 # read from json\ncoor=${opath}/coordinate_minmax.tsv\nIFS=' ' read -r xmin xmax ymin ymax &lt;&lt;&lt;$(cut -d',' -f 5-6 ${brc_raw} | tail -n +2 | awk -v mu=$microns_per_pixel -v FS=',' 'NR == 1 { xmin = $1; xmax = $1; ymin = $2; ymax = $2 } {\\\nif ($1 &gt; xmax) { xmax = $1 }\\\nif ($1 &lt; xmin) { xmin = $1 }\\\nif ($2 &gt; ymax) { ymax = $2 }\\\nif ($2 &lt; ymin) { ymin = $2 }\\\n} END { print xmin*mu, xmax*mu, ymin*mu, ymax*mu }')\necho -e \"xmin\\t${xmin}\\nxmax\\t${xmax}\\nymin\\t${ymin}\\nymax\\t${ymax}\" &gt; ${coor}\n</code></pre> <p>Coordinate range in <code>${opath}/coordinate_minmax.tsv</code> is in micrometer, for the mouse brain data it looks like <pre><code>xmin    0\nxmax    6806.13\nymin    0\nymax    5858.36\n</code></pre></p> <p>Merge coordinate and gene count informations <pre><code># First match barcode index (in the matrix file) with their spatial coordinates (in the tissue_positions file)\n\nawk -v FS=',' -v OFS='\\t' 'NR==FNR{bcd[$1]=NR; next} ($1 in bcd){ printf \"%d\\t%.2f\\t%.2f\\n\", bcd[$1], $2, $3 } ' &lt;(zcat $bfile) &lt;(cut -d',' -f 1,5,6 ${brc_raw}) | sort -k1,1n &gt; ${opath}/barcodes.tsv\n\n# Then annotate the coordinates and gene ID to the matrix file (assume matrix.mtx.gz is sorted by the pixel indices, which seems to be always true)\n\nawk 'BEGIN{FS=OFS=\"\\t\"} NR==FNR{ft[NR]=$1 FS $2; next} ($4 in ft) {print $1, $2, $3, ft[$4], $5 }' \\\n&lt;(zcat $ffile) \\\n&lt;(\\\njoin -t $'\\t' -1 1 -2 2 ${opath}/barcodes.tsv &lt;(zcat $mfile | tail -n +4 | sed 's/ /\\t/g') \\\n) | sort -k3,3n -k2,2n | sed '1 s/^/#barcode_idx\\tX\\tY\\tgene_id\\tgene\\tCount\\n/' | gzip -c &gt; ${output}\n</code></pre></p> <p>(You might want to either delte or zip the intermediate files <code>tissue_positions.raw.csv</code> and <code>barcodes.tsv</code>)</p> <p>The sorting by coordinates part can take some time for large data, you could check intermediate results first to see if it makes sense.</p> <p>Output looks like <pre><code>zcat $output | head\n</code></pre></p> <p><pre><code>#barcode_idx    X       Y       gene_id gene    Count\n4116872 12859.72        318.87  ENSMUSG00000002980      Bcam    1\n4116872 12859.72        318.87  ENSMUSG00000022426      Josd1   1\n4116872 12859.72        318.87  ENSMUSG00000023885      Thbs2   1\n4116872 12859.72        318.87  ENSMUSG00000041120      Nbl1    1\n4116872 12859.72        318.87  ENSMUSG00000072235      Tuba1a  1\n2159571 12867.01        318.98  ENSMUSG00000015090      Ptgds   1\n2159571 12867.01        318.98  ENSMUSG00000028478      Clta    1\n</code></pre> (The first column is just to retain the original barcode index in <code>matrix.mtx.gz</code> (the row number in <code>barcodes.tsv.gz</code>), it will be ignored in analysis.)</p>"},{"location":"format_input/vizgen/","title":"Process Vizgen MERSCOPE data","text":"<p>Locate the transcript file from your Vizgen MERSCOPE output, most likely named as <code>detected_transcripts.csv.gz</code>.</p> <p>The first few lines may look like (from the public Human lung cancer 1 FFPE dataset) <pre><code>,barcode_id,global_x,global_y,global_z,x,y,fov,gene,transcript_id\n747,0,2123.7725,156.28409,1.0,349.0,1891.4949,0,PDK4,ENST00000005178\n846,0,2239.8335,-1.2225803,2.0,1423.6388,433.0998,0,PDK4,ENST00000005178\n2133,0,2133.042,8.58588,5.0,434.82895,523.9189,0,PDK4,ENST00000005178\n</code></pre></p> <p>We will collapse pixels from all z-planes to 2D, essentially using only <code>global_x</code>, <code>global_y</code>, and <code>transcript_id</code>.</p> <p>The following command assume your input is in <code>inpath</code> and you you want to store output to <code>path</code>. In some data the negative control proves are names \"Blank-*\", <code>--dummy_genes Blank</code> will regard any <code>transcript_id</code> containing the substring \"Blank\" as control probes.</p> <p>The script <code>foramt_Vizgen.py</code> can be found in <code>misc/</code> in the FICTURE repository.</p> <pre><code>iden=mouselung # Set how you want to call your dataset\ninput=${inpath}/detected_transcripts.csv.gz\noutput=${path}/filtered.matrix.${iden}.tsv\nfeature=${path}/feature.clean.${iden}.tsv.gz\ncoor=${path}/coordinate_minmax.tsv\n\npython foramt_Vizgen.py --input ${input} --output ${output} --feature ${feature} --coor_minmax ${coor} --precision 2 --dummy_genes Blank\n\nsort -S 4G -k1,1g ${output} | gzip -c &gt; ${output}.gz\nrm ${output}\n</code></pre>"},{"location":"format_input/xenium/","title":"Process 10X xenium data","text":"<p>Locate the transcript file from your Xenium output, most likely named as <code>transcripts.csv.gz</code>.</p> <p>The first few lines may look like (from the public human breast cancer dataset) <pre><code>\"transcript_id\",\"cell_id\",\"overlaps_nucleus\",\"feature_name\",\"x_location\",\"y_location\",\"z_location\",\"qv\"\n281474976710656,565,0,\"SEC11C\",4.395842,328.66647,12.019493,18.66248\n281474976710657,540,0,\"NegControlCodeword_0502\",5.074415,236.96484,7.6085105,18.634956\n</code></pre></p> <p>We will collapse pixels from all z-planes to 2D, essentially using only <code>x_location</code>, <code>y_location</code>, and <code>transcript_id</code>. You may want to keep only transcript with quality score <code>qv</code> above certain threshold.</p> <p>The following command assume your input is in <code>inpath</code> and you you want to store output to <code>path</code>. In some data the negative control proves are names \"Blank-*\", <code>--dummy_genes BLANK\\|NegCon</code> will regard any <code>transcript_id</code> containing the substring \"BLANK\" or \"NegCon\" as control probes. You could provide a regex according to your data.</p> <p>The script <code>format_xenium.py</code> can be found in <code>misc/</code> in the FICTURE repository.</p> <pre><code>input=${inpath}/transcripts.csv.gz\noutput=${path}/filtered.matrix.${iden}.tsv\nfeature=${path}/feature.clean.${iden}.tsv.gz\n\npython format_xenium.py --input ${input} --output ${output} --feature ${feature} --min_phred_score 15 --dummy_genes BLANK\\|NegCon\n\nsort -k2,2g ${output} | gzip -c &gt; ${output}.gz\nrm ${output}\n</code></pre>"}]}